# LLM ローカル環境セットアップ

## 目次

- [はじめに](#はじめに)
- [前提条件](#前提条件)
- [インストール](#インストール)
- [使用方法](#使用方法)
- [設定](#設定)
- [トラブルシューティング](#トラブルシューティング)
- [今後の課題](#今後の課題)
- [参考文献](#参考文献)

## はじめに

日経ソフトウェア9月号 特集1 ローカルLLM入門 の記事を実際に試した結果を記録するものです。

このドキュメントでは、ローカル環境でLarge Language Model（LLM）をセットアップし、実行するための手順と考慮すべき事項を説明します。

本学習の目的は、LLMのローカル環境でのデプロイメントと管理に関する実践的な経験を積むことです。

## Part1 LLMをローカル環境で動かす方法

- LLMのファイル https://huggingface.co/models

### LLM選定のポイント

- .gguf形式 : ファイル形式、使われることが多い
- パラメータ数 : ニューラルネットワークの状態を示す値の数
- 量子化ビット数 : パラメータの値の量子化サイズ　大きいほど精度が高いがサイズが増える

### LLMプラットフォーム

- Jan
  - https://jan.ai/

## 前提条件

開始する前に、以下のものがマシンにインストールされ、設定されていることを確認してください:
- [ ] **Python 3.8以上**: LLMを実行するためのプログラミング言語。
- [ ] **仮想環境**: 依存関係を管理するために必要です。
- [ ] **GPU（推奨）**: 大規模な計算リソースを必要とするモデルを扱う場合に推奨されます。
- [ ] **Docker**: コンテナ化されたデプロイメントに必要です（該当する場合）。
- [ ] **Git**: リポジトリをクローンするためのバージョン管理システム。

## インストール
### ステップ 1: リポジトリをクローン
```bash
git clone <repository-url>
cd <repository-directory>

python -m venv llm_env
source llm_env/bin/activate  # Windowsの場合は `llm_env\Scripts\activate`
